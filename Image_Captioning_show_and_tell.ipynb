{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Image Captioning show and tell.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHKB_0vjJ9BH",
        "colab_type": "text"
      },
      "source": [
        "## **Vision et language , Image Captioning show and tell**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIV6ssWPJ67J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install  --upgrade grpcio\n",
        "!pip uninstall tensorflow==1.15.0\n",
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB7w17EuCASv",
        "colab_type": "text"
      },
      "source": [
        "## **Exercice 1 : Simplification du vocabulaire**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFf4bhQ1DVPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb5rzBAeBeUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import _pickle as pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "filename = 'flickr_8k_train_dataset.txt'\n",
        "df = pd.read_csv(filename, delimiter='\\t')\n",
        "nb_samples = df.shape[0]\n",
        "iter = df.iterrows()\n",
        "\n",
        "bow = {}\n",
        "nbwords = 0\n",
        "\n",
        "for i in range(nb_samples):\n",
        " x = iter.__next__()\n",
        " cap_words = x[1][1].split() # split caption into words\n",
        " cap_wordsl = [w.lower() for w in cap_words] # remove capital letters\n",
        " nbwords += len(cap_wordsl)\n",
        " for w in cap_wordsl:\n",
        "  if (w in bow):\n",
        "   bow[w] = bow[w]+1\n",
        "  else:\n",
        "   bow[w] = 1\n",
        "\n",
        "bown = sorted([(value,key) for (key,value) in bow.items()], reverse=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRsxO--JDBs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://cedric.cnam.fr/~thomen/cours/US330X/Caption_Embeddings.p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJItsVeEDaQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbkeep = 1000 # 100 is needed for fast processing\n",
        "import pandas as pd\n",
        "outfile = 'Caption_Embeddings.p'\n",
        "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
        "df = pd.DataFrame(listwords)\n",
        "\n",
        "embeddings_new = np.zeros((nbkeep,102))\n",
        "listwords_new = []\n",
        "print(type(listwords_new))\n",
        "for i in range(nbkeep):\n",
        " listwords_new.append(bown[i][1])\n",
        " embeddings_new[i,:] =  embeddings[np.where(df==bown[i][1])[0][0] ,:]# COMPLETE WITH YOUR CODE\n",
        " embeddings_new[i,:] /= np.linalg.norm(embeddings_new[i,:]) # Normalization\n",
        "\n",
        "\n",
        "listwords = listwords_new\n",
        "embeddings = embeddings_new\n",
        "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
        "with open(outfile, \"wb\" ) as pickle_f:\n",
        " pickle.dump( [listwords, embeddings], pickle_f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVWQCgUbOANm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freqnc = np.cumsum([float(w[0])/nbwords*100.0 for w in bown])\n",
        "\n",
        "x_axis = [str(bown[i][1]) for i in range(100)]\n",
        "plt.figure(dpi=300)\n",
        "plt.xticks(rotation=90, fontsize=3)\n",
        "plt.ylabel('Word Frequency')\n",
        "plt.bar(x_axis, freqnc[0:100])\n",
        "\n",
        "print(\"number of kept words=\"+str(nbkeep)+\" - ratio=\"+str(freqnc[nbkeep-1])+\" %\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjz7XQhhSeNe",
        "colab_type": "text"
      },
      "source": [
        "## **Exercice 2 : Création des données d’apprentissage et de test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOEjWgpRSZAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_train_dataset.txt\n",
        "!wget http://cedric.cnam.fr/~thomen/cours/US330X/flickr_8k_test_dataset.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCyuWNdaSmM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'flickr_8k_train_dataset.txt'\n",
        "df = pd.read_csv(filename, delimiter='\\t')\n",
        "nbTrain = df.shape[0]\n",
        "iter = df.iterrows()\n",
        "\n",
        "caps = [] # Set of captions\n",
        "imgs = [] # Set of images\n",
        "for i in range(nbTrain):\n",
        " x = iter.__next__()\n",
        " caps.append(x[1][1])\n",
        " imgs.append(x[1][0])\n",
        "\n",
        "maxLCap = 0\n",
        "\n",
        "for caption in caps:\n",
        "   l=0\n",
        "   words_in_caption =  caption.split()\n",
        "   for j in range(len(words_in_caption)-1):\n",
        "    current_w = words_in_caption[j].lower()\n",
        "    if(current_w in listwords):\n",
        "     l+=1\n",
        "   if(l > maxLCap):\n",
        "       maxLCap = l\n",
        "\n",
        "   print(\"max caption length =\"+str(maxLCap))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0_e4PmtTID6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://cedric.cnam.fr/~thomen/cours/US330X/encoded_images_PCA.p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MRIcBy3fNgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
        "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) ) # Loading reduced dictionary\n",
        "indexwords = {} # Useful for tensor filling\n",
        "for i in range(len(listwords)):\n",
        " indexwords[listwords[i]] = i\n",
        "\n",
        "# Loading images features\n",
        "encoded_images = pickle.load( open( \"encoded_images_PCA.p\", \"rb\" ) )\n",
        "\n",
        "# Allocating data and labels tensors\n",
        "tinput = 202\n",
        "tVocabulary = len(listwords)\n",
        "X_train = np.zeros((nbTrain,maxLCap, tinput))\n",
        "Y_train = np.zeros((nbTrain,maxLCap, tVocabulary), bool)\n",
        "\n",
        "for i in range(nbTrain):\n",
        " words_in_caption =  caps[i].split()\n",
        " indseq=0 # current sequence index (to handle mising words in reduced dictionary)\n",
        " for j in range(len(words_in_caption)-1):\n",
        "  current_w = words_in_caption[j].lower()\n",
        "  if(current_w in listwords):\n",
        "   X_train[i,indseq,0:100] = encoded_images[imgs[i]]# COMPLETE WITH YOUR CODE\n",
        "   ind = listwords.index(current_w)\n",
        "   X_train[i,indseq,100:202] = embeddings[ind,:]# COMPLETE WITH YOUR CODE\n",
        "\n",
        "  next_w = words_in_caption[j+1].lower()\n",
        "  if(next_w in listwords):\n",
        "   index_pred = indexwords[next_w]# COMPLETE WITH YOUR CODE\n",
        "   Y_train[i,indseq,index_pred] = 1# COMPLETE WITH YOUR CODE\n",
        "   indseq += 1 # Increment index if target label present in reduced dictionary\n",
        "\n",
        "outfile = 'Training_data_'+str(nbkeep)\n",
        "np.savez(outfile, X_train=X_train, Y_train=Y_train) # Saving tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yirJrHZMhhmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'flickr_8k_test_dataset.txt'\n",
        "df = pd.read_csv(filename, delimiter='\\t')\n",
        "nbTest = df.shape[0]\n",
        "iter = df.iterrows()\n",
        "\n",
        "caps = [] # Set of captions\n",
        "imgs = [] # Set of images\n",
        "for i in range(nbTest):\n",
        "    x = iter.__next__()\n",
        "    caps.append(x[1][1])\n",
        "    imgs.append(x[1][0])\n",
        "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
        "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) ) # Loading reduced dictionary\n",
        "indexwords = {} # Useful for tensor filling\n",
        "for i in range(len(listwords)):\n",
        "    indexwords[listwords[i]] = i\n",
        "\n",
        "# Loading images features\n",
        "encoded_images = pickle.load( open( \"encoded_images_PCA.p\", \"rb\" ) )\n",
        "\n",
        "# Allocating data and labels tensors\n",
        "tinput = 202\n",
        "tVocabulary = len(listwords)\n",
        "X_test = np.zeros((nbTest,maxLCap, tinput))\n",
        "Y_test = np.zeros((nbTest,maxLCap, tVocabulary), bool)\n",
        "\n",
        "for i in range(nbTest):\n",
        "    words_in_caption =  caps[i].split()\n",
        "    indseq=0 # current sequence index (to handle mising words in reduced dictionary)\n",
        "    for j in range(len(words_in_caption)-1):\n",
        "        current_w = words_in_caption[j].lower()\n",
        "        if(current_w in listwords):\n",
        "            X_test[i,indseq,0:100] = encoded_images[imgs[i]]# COMPLETE WITH YOUR CODE\n",
        "            ind = listwords.index(current_w)\n",
        "            X_test[i,indseq,100:202] = embeddings[ind,:]# COMPLETE WITH YOUR CODE\n",
        "\n",
        "        next_w = words_in_caption[j+1].lower()\n",
        "        if(next_w in listwords):\n",
        "            index_pred = indexwords[next_w] # COMPLETE WITH YOUR CODE\n",
        "            Y_test[i,indseq,index_pred] = True# COMPLETE WITH YOUR CODE\n",
        "            indseq += 1 # Increment index if target label present in reduced dictionary\n",
        "\n",
        "outfile = 'Test_data_'+str(nbkeep)\n",
        "np.savez(outfile, X_test=X_test, Y_test=Y_test) # Saving tensor    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx82jI20iTZN",
        "colab_type": "text"
      },
      "source": [
        "## **Exercice 3 : Entraînement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psESTM5RiPjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "SEQLEN = 35\n",
        "taille_chars = 202\n",
        "HSIZE = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=0.0, input_shape=(SEQLEN, taille_chars)))\n",
        "model.add(SimpleRNN(HSIZE, return_sequences=True, input_shape=(SEQLEN, taille_chars), unroll=True))\n",
        "model.add(Dense(1000, name='fc1'))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "BATCH_SIZE = 10\n",
        "NUM_EPOCHS = 10\n",
        "#learning_rate = 1.0\n",
        "#optim = Adam(learning_rate)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer='Adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS)\n",
        "\n",
        "scores_train = model.evaluate(X_train, Y_train, verbose=1)\n",
        "scores_test = model.evaluate(X_test, Y_test, verbose=1)\n",
        "print(\"PERFS TRAIN: %s: %.2f%%\" % (model.metrics_names[1], scores_train[1]*100))\n",
        "print(\"PERFS TEST: %s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYrHPLPL_RlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qnlr27pjjGRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import model_from_yaml\n",
        "import os\n",
        "def saveModel(model, savename):\n",
        "  model.save(os.path.join(\".\",savename+\".h5\"))\n",
        "  print (\"Weights \",savename,\".h5 saved to disk\")\n",
        "\n",
        "saveModel(model,\"my_model_RNN\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdqcLxH1k8c-",
        "colab_type": "text"
      },
      "source": [
        "# **Exercice 4 : Évaluation du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJWnnr44lHcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://cedric.cnam.fr/~thomen/cours/US330X/Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_Dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btj-NMaxkgIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "nameModel = \"my_model_RNN.h5\"\n",
        "model = load_model(os.path.join(\".\",nameModel))\n",
        "\n",
        "optim = Adam()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optim,metrics=['accuracy'])\n",
        "\n",
        "# LOADING TEST DATA\n",
        "outfile = 'Test_data_'+str(nbkeep)+'.npz'\n",
        "npzfile = np.load(outfile)\n",
        "\n",
        "X_test = npzfile['X_test']\n",
        "Y_test = npzfile['Y_test']\n",
        "\n",
        "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
        "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
        "indexwords = {}\n",
        "for i in range(len(listwords)):\n",
        " indexwords[listwords[i]] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2pc0zk7mAc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "\n",
        "ind = np.random.randint(X_test.shape[0])\n",
        "\n",
        "filename = 'flickr_8k_test_dataset.txt' #  PATH IF NEEDED\n",
        "\n",
        "df = pd.read_csv(filename, delimiter='\\t')\n",
        "iter = df.iterrows()\n",
        "\n",
        "for i in range(ind+1):\n",
        " x = iter.__next__()\n",
        "\n",
        "imname = x[1][0]\n",
        "print(\"image name=\"+imname+\" caption=\"+x[1][1])\n",
        "dirIm = \"Flicker8k_Dataset/\" # CHANGE WITH YOUR DATASET\n",
        "\n",
        "img=mpimg.imread(dirIm+imname)\n",
        "plt.figure(dpi=100)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AnXje6xmwuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(X_test[ind:ind+1,:,:])\n",
        "def sampling(preds, temperature=1.0):\n",
        " preds = np.asarray(preds).astype('float64')\n",
        " predsN = pow(preds,1.0/temperature)\n",
        " predsN /= np.sum(predsN)\n",
        " probas = np.random.multinomial(1, predsN, 1)\n",
        " return np.argmax(probas)\n",
        "\n",
        "nbGen = 5\n",
        "temperature=0.1 # Temperature param for peacking soft-max distribution\n",
        "\n",
        "for s in range(nbGen):\n",
        " wordpreds = \"Caption n° \"+str(s+1)+\": \"\n",
        " indpred = sampling(pred[0,0,:], temperature)\n",
        " wordpred = listwords[indpred]\n",
        " wordpreds +=str(wordpred)+ \" \"\n",
        " X_test[ind:ind+1,1,100:202] = embeddings[listwords.index(wordpred)]# COMPLETE WITH YOUR CODE\n",
        " cpt=1\n",
        " while(str(wordpred)!='<end>' and cpt<30):\n",
        "  pred = model.predict(X_test[ind:ind+1,:,:])\n",
        "  indpred = sampling(pred[0,cpt,:], temperature)\n",
        "  wordpred = listwords[indpred]\n",
        "  #list_refpredict.append(wordpred) \n",
        "  wordpreds += str(wordpred)+ \" \"\n",
        "  cpt+=1\n",
        "  X_test[ind:ind+1,cpt,100:202] = embeddings[listwords.index(wordpred)]# COMPLETE WITH YOUR CODE\n",
        " print(wordpreds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_QihuGIE-3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbTest/5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epcyGi86oJgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from tensorflow.keras.models import model_from_yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "# LOADING TEST DATA\n",
        "nbkeep = 1000\n",
        "#outfile =  # REPLACE WITH YOUR DATA PATH\n",
        "outfile = 'Test_data_'+str(nbkeep)+'.npz'\n",
        "npzfile = np.load(outfile)\n",
        "\n",
        "X_test = npzfile['X_test']\n",
        "Y_test = npzfile['Y_test']\n",
        "\n",
        "# LOADING MODEL\n",
        "# nameModel = \"my_model_RNN.h5\" #REPLACE WITH YOUR MODEL NAME\n",
        "# model = loadModel(nameModel)\n",
        "\n",
        "# COMPILING MODEL\n",
        "optim = Adam()\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optim,metrics=['accuracy'])\n",
        "scores_test = model.evaluate(X_test, Y_test, verbose=1)\n",
        "print(\"PERFS TEST: %s: %.2f%%\" % (model.metrics_names[1], scores_test[1]*100))\n",
        "\n",
        "# LOADING TEXT EMBEDDINGS\n",
        "outfile = \"Caption_Embeddings_\"+str(nbkeep)+\".p\"\n",
        "[listwords, embeddings] = pickle.load( open( outfile, \"rb\" ) )\n",
        "indexwords = {}\n",
        "for i in range(len(listwords)):\n",
        " indexwords[listwords[i]] = i\n",
        "\n",
        "# COMPUTING CAPTION PREDICTIONS ON TEST SET\n",
        "predictions = []\n",
        "nbTest = X_test.shape[0]\n",
        "for i in range(0,nbTest,5):\n",
        " pred = model.predict(X_test[i:i+1,:,:])\n",
        " wordpreds = []\n",
        " indpred = np.argmax(pred[0,0,:])\n",
        " wordpred = listwords[indpred]\n",
        " wordpreds.append(str(wordpred))\n",
        " X_test[i,1,100:202] = embeddings[indpred]\n",
        " cpt=1\n",
        " while(str(wordpred)!='<end>' and cpt<(X_test.shape[1]-1)):\n",
        "  pred = model.predict(X_test[i:i+1,:,:])\n",
        "  indpred = np.argmax(pred[0,cpt,:])\n",
        "  wordpred = listwords[indpred]\n",
        "  if(wordpred !='<end>'):\n",
        "   wordpreds.append(str(wordpred))\n",
        "  cpt+=1\n",
        "  X_test[i,cpt,100:202] = embeddings[indpred]\n",
        "\n",
        " if(i%1000==0):\n",
        "  print(\"i=\"+str(i)+\" \"+str(wordpreds))\n",
        " predictions.append(wordpreds)\n",
        "\n",
        "# LOADING GROUD TRUTH CAPTIONS ON TEST SET\n",
        "references = []\n",
        "filename = 'flickr_8k_test_dataset.txt'\n",
        "df = pd.read_csv(filename, delimiter='\\t')\n",
        "iter = df.iterrows()\n",
        "\n",
        "ccpt =0\n",
        "xx= nbTest/5\n",
        "for i in range(xx):\n",
        " captions_image = []\n",
        " for j in range(5):\n",
        "  x = iter.__next__()\n",
        "  ll = x[1][1].split()\n",
        "  caption = []\n",
        "  for k in range(1,len(ll)-1):\n",
        "   caption.append(ll[k])\n",
        "\n",
        "  captions_image.append(caption)\n",
        "  ccpt+=1\n",
        "\n",
        " references.append(captions_image)\n",
        "\n",
        "# COMPUTING BLUE-1, BLUE-2, BLUE-3, BLUE-4\n",
        "blue_scores = np.zeros(4)\n",
        "weights = np.zeros((4,4))\n",
        "weights[0,0] = 1\n",
        "weights[1,0] = 0.5\n",
        "weights[1,1] = 0.5\n",
        "weights[2,0] = 1.0/3.0\n",
        "weights[2,1] = 1.0/3.0\n",
        "weights[2,2] = 1.0/3.0\n",
        "weights[3,:] = 1.0/4.0\n",
        "\n",
        "for i in range(4):\n",
        " blue_scores[i] = nltk.translate.bleu_score.corpus_bleu(references, predictions, weights = (weights[i,0], weights[i,1], weights[i,2], weights[i,3]) )\n",
        " print(\"blue_score - \"+str(i)+\"=\"+str( blue_scores[i]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md_uJZpkocxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}